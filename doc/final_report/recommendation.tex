\section{Proposed Methods}
\subsection{Simple Matching}
To begin with, we proposed a simple matching method as our baseline. It requires no previous knowledge of training samples. Given a test sample, the algorithm simply tries to match each word in the test sample with the list of tags. If the word exists in the tag list, the algorithm put this word as one of the candidate tags. Then the candidate tag list is sorted in descending order based on the frequency of tags in the train set, denoted as $F(Y)$, and output the top $k$ tags in the list. If the list is not full, then the algorithm fills the list up by picking tags in the remaining tag list with maximum $F(Y)$. The algorithm is described in Algorithm \ref{alg:sm}.

\IncMargin{1em}
\begin{algorithm}
\label{alg:sm}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Frequency of tags $F(Y)$, constant $k$, test case $X_t$}
\Output{Sorted list of predicted tags $L$ for $X_t$}
\BlankLine
Initialize $L \leftarrow \emptyset$ \;
\ForEach{$x_i \in X_t$} {
	\If{$F(x_i) \neq 0$}{
		put $x_i$ in $L$ \;
	}
}
Sort $l_i \in L$ in descending order by $F(l_i)$ \;
\If{$size(L)>k$}{
	Remove all elements $l_i$ that $i \geq k$ \;
}
\While{$size(L)<k$}{
	Pick a tag in the remaining tags with maximum frequency \;
}
\Return $L$\;
\caption{Simple Matching Algorithm}\label{algo_disjdecomp}
\end{algorithm}
\DecMargin{1em}

\subsection{Naive Bayes}
Naive Bayes classifier is a simple yet powerful classifier based on Bayes theorem. It assumes that the presence of one feature is independent from the presence of the others. When performing the text categorization, Naive Bayes treats each document as a ``bag of words" and words are conditionally independent from each other.

In practice, naive Bayes classifiers can have a very satisfactory performance in a supervised learning. Many real-world problems are tackled by naive Bayes. Owing to its simplicity and desirable accuracy in many cases, we chose naive Bayes as our baseline. In our tagging prediction problem, we view the words as the features and tag the labels.

By applying Bayes theorem, we can get the label $Y$'s probability given $X$.
\begin{gather}
    P(Y \vert X_1,\dots,X_n) = \frac{P(Y) \ P(X_1,\dots,X_n\vert Y)}{P(X_1,\dots,X_n)}. 
\end{gather}

With the independence assumption, the classification problem is equivalent to:
\begin{gather}
    \mathrm{y} = \underset{y}{\operatorname{argmax}} \ P(Y=y) \displaystyle\prod_{i=1}^n P(X_i=x_i\vert Y=y).
\end{gather}
The training algorithm is stated in Algorithm \ref{alg:nb}.

\IncMargin{1em}
\begin{algorithm}
\label{alg:nb}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Training set $T=\{t_1,...t_n\}$}
\Output{$P(Y)$ and  $P(X|Y)$}
\BlankLine
Initialize all $Count(y)$ and $Count(x|y)$ to be 0\;
Initialize total count $N$ to be 0\;

\ForEach{$t \in T$} {
    $X_t, y_t$ = $t$\;
    \ForEach{$x \in X_t$} {
        $Count(y) \leftarrow Count(x) + 1$ \;
        $Count(x|y) \leftarrow Count(x|y) + 1$ \;
        $N \leftarrow N$ + 1 \;
    }
}
% Calculate $P(Y), P(X|Y)$ from $Count(Y), Count(X|Y)$ and $N$}
\Return $P(Y), P(X|Y) \leftarrow CalculateProbability(Count(Y), Count(X|Y), N)$

\caption{Naive Bayes Training Algorithm}\label{algo_disjdecomp}
\end{algorithm}
\DecMargin{1em}

With the calculated $P(Y) and P(X|Y)$, we can apply the MLE(or MAP) to find the top-rank tags.

\subsection{k-Nearest Neighbor}

The $k$-nearest neighbor algorithm\footnote{http://en.wikipedia.org/wiki/K-nearest\_neighbor\_algorithm} ($k$-NN) is a method for classifying objects based on closest training examples in the feature space. It is type of instance-based learning where the function is only approximated locally and all computation is deferred until classification. The training phase of this algorithm consist only of storing the feature vectors and the classification labels. In the classification phase, a test case is assigned with the most frequent labels among the the nearest $k$ training samples. The distance between two feature vectors is measured by some specific metrics and $k$ is a user-defined constant value.

By applying this method to our problem, our intuition is that similar questions will have similar tags. We use cosine similarity to measure the closeness of two feature vectors. The definition is as follows:

\begin{gather}
	CosSim(P,Q)=\frac{\sum_{t \in V}w_P(t)w_Q(t)}{\sqrt{\sum_{t \in V}w_P(t)^2 \sum_{t \in V}w_Q(t)^2}}
\end{gather}

where $w_P(t)$ and $w_Q(t)$ are the weights of the term $t$ in feature vector $P$ and $Q$, respectively. We remove stopwords and stem the remaining tokens before computing the similarity score. And we use \textbf{lnn} term weights (i.e. logarithmic term frequency, no tag frequency, no normalization). For example, $w_P(t) = 1 + log(f_P(t))$ where $f_P(t)$ is the frequency count of term $t$ in vector $P$. The algorithm for k-NN is stated in Algorithm \ref{alg:knn}.

\IncMargin{1em}
\begin{algorithm}
\label{alg:knn}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Training set $T=\{X_1,...X_n\}$, constant $k$, test case $X_t$}
\Output{Sorted list of predicted tags $L$ for $X_t$}
\BlankLine
Initialize $L \leftarrow \emptyset$ and $scoreList \leftarrow \emptyset$\;
\ForEach{$X_i \in T$} {
	$score \leftarrow CosSim(X_i, X_t)$ \;
	$scoreList_i \leftarrow score$ \;
}
Sort $s_i \in scoreList$ in descending order \; 
Put the tag of $s_0, s_1, ..., s_{k-1}$ in $L$ \;
\Return $L$\;
\caption{k-Nearest Neighbor Algorithm}\label{algo_disjdecomp}
\end{algorithm}
\DecMargin{1em}

