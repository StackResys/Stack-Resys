
\subsection{Naive Bayes}
Naive Bayes classifier is a simple yet powerful classifier based on Bayes theorem. It assumes that the presence of one feature is independent from the presence of the others. When performing the text categorization, Naive Bayes treats each document as a "bag of words" and words are conditionally independent from each other.

In practice, naive Bayes classifiers can have a very satisfactory performance in a supervised learning. Many real-world problems are tackled by naive Bayes. Owing to its simplicity and desirable accuracy in many cases, we chose naive Bayes as our baseline.

By applying Bayes theorem, we can get the label $Y$'s probability given $X$.
\begin{gather}
    P(Y \vert X_1,\dots,X_n) = \frac{P(Y) \ P(X_1,\dots,X_n\vert Y)}{P(X_1,\dots,X_n)}. 
\end{gather}

With the independence assumption, the classification problem is equivalent to:
\begin{gather}
    \mathrm{y} = \underset{y}{\operatorname{argmax}} \ P(Y=y) \displaystyle\prod_{i=1}^n P(X_i=x_i\vert Y=y).
\end{gather}
The training algorithm is stated in Algorithm\ref{alg:nb}.

\IncMargin{1em}
\begin{algorithm}
\label{alg:nb}
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Training set $T=\{t_1,...t_n\}$}
\Output{$P(Y)$ and  $P(X|Y)$}
\BlankLine
Initialize all $Count(y)$ and $Count(x|y)$ to be 0\;
Initialize total count $N$ to be 0\;

\ForEach{$t \in T$} {
    $X_t, y_t$ = $t$\;
    \ForEach{$x \in X_t$} {
        $Count(y) \leftarrow Count(x) + 1$ \;
        $Count(x|y) \leftarrow Count(x|y) + 1$ \;
        $N \leftarrow N$ + 1 \;
    }
}
% Calculate $P(Y), P(X|Y)$ from $Count(Y), Count(X|Y)$ and $N$}
\Return $P(Y), P(X|Y) \leftarrow CalculateProbability(Count(Y), Count(X|Y), N)$

\caption{Naive Bayes Training Algorithm}\label{algo_disjdecomp}
\end{algorithm}
\DecMargin{1em}

\subsection{Logistic Regression}

Logistic regression is a discriminative model which learns $P(Y|X)$ directly from the training data. In our problem the value of $y$ takes any of the discrete values $\{y_1,...y_K\}$, and the form of $P(Y=y_k|X)$ for $Y=y_1,...Y=y_{K-1}$ is: 

\begin{gather}
	P(Y=y_k|X)=\frac{exp(w_{k0}+\sum_{i=1}^n{w_{ki}X_i})}{1+\sum_{j=1}^{K-1}exp(w_{j0}+\sum_{i=1}^n{w_{ji}X_i})}
\end{gather}

For $Y=y_K$, the form is:
\begin{gather}
	P(Y=y_K|X)=\frac{1}{1+\sum_{j=1}^{K-1}exp(w_{j0}+\sum_{i=1}^n{w_{ji}X_i})}
\end{gather}

Here $X_i$ denotes the $i$th variable in $X$, and $w_{ji}$ means the weight of $j$th class of $Y$ with variable $X_i$.

If using gradient descent rule with regularization in order to estimate the values of $w_{ji}$, we are after:
\begin{gather}
	w_{ji} \leftarrow w_{ji}+ \eta \sum_{l}X_{i}^{l}(\delta (y_{j} \in Y^{l})-\hat{P}(y_{j} \in Y^{l}|X^{l},W))- \eta \lambda w_{ji}
\end{gather}
where $\eta$ is a small constant which determines the step size, and $\lambda$ is the regularization constant. The algorithm is stated in Algorithm \ref{alg:lr}.

\IncMargin{1em}
\begin{algorithm}
\label{alg:lr}
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Training set $T=\{t_1,...t_n\}$, constant $\eta$, converge threshold $\varepsilon$, regulation factor $\lambda$}
\Output{Weight matrix $W$}
\BlankLine
Initialize all $w_{ji} \in W$ to 0\;
$isConverge \leftarrow false$\;
\While{$isConverge = false$}{
	\ForEach{$w_{ji} \in W$}{
		\ForEach{$t_l \in T$}{
			$jump_{ji} \leftarrow 0$\;
			Calculate $d=X_{i}^{l}(\delta (y_{j} \in Y^{l})-\hat{P}(y_{j} \in Y^{l}|X^{l},W))$\;
			$jump_{ji} \leftarrow \eta * d$\;
		}
		$w_{ji} \leftarrow w_{ji} + jump_{ji}$\;
	}
	$w_{ji} \leftarrow w_{ji} - \eta \lambda w_{ji}$\;
	\If{$\forall jump_{ji} \rightarrow jump_{ji} < \varepsilon$}{
		$isConverge \leftarrow true$\;
	}
}
\Return $W$\;
\caption{Logistic Regression}\label{algo_disjdecomp}
\end{algorithm}
\DecMargin{1em}

% TODO: CURIOUS CASE OF STRANGE LAYOUT
% \pagebreak

\subsection{SVM}
