
\subsection{Naive Bayes}
Naive Bayes classifier is a simple yet powerful classifier based on Bayes theorem. It assumes that the presence of one feature is independent from the presence of the others. When performing the text categorization, Naive Bayes treats each document as a ``bag of words" and words are conditionally independent from each other.

In practice, naive Bayes classifiers can have a very satisfactory performance in a supervised learning. Many real-world problems are tackled by naive Bayes. Owing to its simplicity and desirable accuracy in many cases, we chose naive Bayes as our baseline.

By applying Bayes theorem, we can get the label $Y$'s probability given $X$.
\begin{gather}
    P(Y \vert X_1,\dots,X_n) = \frac{P(Y) \ P(X_1,\dots,X_n\vert Y)}{P(X_1,\dots,X_n)}. 
\end{gather}

With the independence assumption, the classification problem is equivalent to:
\begin{gather}
    \mathrm{y} = \underset{y}{\operatorname{argmax}} \ P(Y=y) \displaystyle\prod_{i=1}^n P(X_i=x_i\vert Y=y).
\end{gather}
The training algorithm is stated in Algorithm\ref{alg:nb}.

\IncMargin{1em}
\begin{algorithm}
\label{alg:nb}
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Training set $T=\{t_1,...t_n\}$}
\Output{$P(Y)$ and  $P(X|Y)$}
\BlankLine
Initialize all $Count(y)$ and $Count(x|y)$ to be 0\;
Initialize total count $N$ to be 0\;

\ForEach{$t \in T$} {
    $X_t, y_t$ = $t$\;
    \ForEach{$x \in X_t$} {
        $Count(y) \leftarrow Count(x) + 1$ \;
        $Count(x|y) \leftarrow Count(x|y) + 1$ \;
        $N \leftarrow N$ + 1 \;
    }
}
% Calculate $P(Y), P(X|Y)$ from $Count(Y), Count(X|Y)$ and $N$}
\Return $P(Y), P(X|Y) \leftarrow CalculateProbability(Count(Y), Count(X|Y), N)$

\caption{Naive Bayes Training Algorithm}\label{algo_disjdecomp}
\end{algorithm}
\DecMargin{1em}

\subsection{Logistic Regression}

Logistic regression is a discriminative model which learns $P(Y|X)$ directly from the training data. In our problem the value of $y$ takes any of the discrete values $\{y_1,...y_K\}$, and the form of $P(Y=y_k|X)$ for $Y=y_1,...Y=y_{K-1}$ is: 

\begin{gather}
	P(Y=y_k|X)=\frac{exp(w_{k0}+\sum_{i=1}^n{w_{ki}X_i})}{1+\sum_{j=1}^{K-1}exp(w_{j0}+\sum_{i=1}^n{w_{ji}X_i})}
\end{gather}

For $Y=y_K$, the form is:
\begin{gather}
	P(Y=y_K|X)=\frac{1}{1+\sum_{j=1}^{K-1}exp(w_{j0}+\sum_{i=1}^n{w_{ji}X_i})}
\end{gather}

Here $X_i$ denotes the $i$th variable in $X$, and $w_{ji}$ means the weight of $j$th class of $Y$ with variable $X_i$.

If using gradient descent rule with regularization in order to estimate the values of $w_{ji}$, we are after:
\begin{gather}
	w_{ji} \leftarrow w_{ji}+ \eta \sum_{l}X_{i}^{l}(\delta (y_{j} \in Y^{l})-\hat{P}(y_{j} \in Y^{l}|X^{l},W))- \eta \lambda w_{ji}
\end{gather}
where $\eta$ is a small constant which determines the step size, and $\lambda$ is the regularization constant. The algorithm is stated in Algorithm \ref{alg:lr}.

\IncMargin{1em}
\begin{algorithm}
\label{alg:lr}
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Training set $T=\{t_1,...t_n\}$, constant $\eta$, converge threshold $\varepsilon$, regulation factor $\lambda$}
\Output{Weight matrix $W$}
\BlankLine
Initialize all $w_{ji} \in W$ to 0\;
$isConverge \leftarrow false$\;
\While{$isConverge = false$}{
	\ForEach{$w_{ji} \in W$}{
		\ForEach{$t_l \in T$}{
			$jump_{ji} \leftarrow 0$\;
			Calculate $d=X_{i}^{l}(\delta (y_{j} \in Y^{l})-\hat{P}(y_{j} \in Y^{l}|X^{l},W))$\;
			$jump_{ji} \leftarrow \eta * d$\;
		}
		$w_{ji} \leftarrow w_{ji} + jump_{ji}$\;
	}
	$w_{ji} \leftarrow w_{ji} - \eta \lambda w_{ji}$\;
	\If{$\forall jump_{ji} \rightarrow jump_{ji} < \varepsilon$}{
		$isConverge \leftarrow true$\;
	}
}
\Return $W$\;
\caption{Logistic Regression Training Algorithm}\label{algo_disjdecomp}
\end{algorithm}
\DecMargin{1em}

% TODO: CURIOUS CASE OF STRANGE LAYOUT
% \pagebreak

\subsection{Neural Networks}

Neural Networks is another classifier model that can be applied to our problem. In our case, the input units represent words in the question, and output units represent tags. We add one hidden layer in this network (normally one hidden layer is enough), and use backpropagation to train the weights. 

Our algorithm for training the Neural Networks is stated in Algorithm \ref{alg:nn}. Each training sample is a vector pair $<\overrightarrow{x},\overrightarrow{y}>$, where $\overrightarrow{x}$ is the vector of network input values, and $\overrightarrow{y}$ is the target network output values. $\eta$ is a small constant set to 0.05. $n_w$, $n_h$ and $n_t$ stand for the number of words, the number of hidden units and the number of tags, respectively. $w_{wh}$ is the weight from a word unit to a hidden unit, and $w_{ht}$ is the weight from a hidden unit to a tag unit. The algorithm outputs two weight matrix consisting all the weights of $w_{wh}$ and $w_{ht}$.

\IncMargin{1em}
\begin{algorithm}
\label{alg:nn}
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Training set $T=\{t_1,...t_n\}$, constant $\eta$, $n_w$, $n_h$, $n_t$}
\Output{Two weight matrix $W_{wh}$ and $W_{hy}$}
\BlankLine
Create a feed-forward neural network with $n_w$ input units, $n_h$ hidden units and $n_t$ output units\;
Initialize all $w_{wh} \in W_{wh}$ and $w_{ht} \in W_{ht}$ to be small random numbers between $(-0.05, 0.05)$\;
\ForEach{Training sample $<\overrightarrow{x},\overrightarrow{y}>$}{
	Compute $O_{w}^{l}=\sigma (\overrightarrow{w}*\overrightarrow{x})$ and $O_{h}^{l}=\sigma (\overrightarrow{w}*\overrightarrow{O_{w}^{l}})$ 
		where $\sigma (y)=\frac{1}{1+exp\{-y\}}$ \;
	For each $O_{w}^{l}$, compute $\delta_{w}^{l} \leftarrow O_{w}^{l} (1-O_{w}^{l}) (t_{w}^{l} - O_{w}^{l})$ \;
	For each $O_{h}^{l}$, compute $\delta_{h}^{l} \leftarrow O_{h}^{l} (1-O_{h}^{l}) \sum_{w \in outputs}{w_i \delta_{k}^{l}}$ \;
	For each $w_{wh} \in W_{wh}$, compute $w_{wh} \leftarrow w_{wh}+\eta \delta_{w}^{l}O_{w}^{l}$ \;
	For each $w_{ht} \in W_{ht}$, compute $w_{ht} \leftarrow w_{ht}+\eta \delta_{h}^{l}O_{h}^{l}$ \;
}
\Return $W_{wh}$ and $W_{ht}$ \;

\caption{Neural Networks Training Algorithm}\label{algo_disjdecomp}
\end{algorithm}\DecMargin{1em}
